{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "#import os\n",
    "#import re\n",
    "import sys\n",
    "import json\n",
    "#import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "#modelfile = wget.download(udpipe_url)\n",
    "modelfile = 'udpipe_syntagrus.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TODO:  12-stuljev\n",
      "TODO:  pohozhdeniya-bravogo-soldata-shveyka\n",
      "TODO:  zapiski sumasshedshego\n",
      "TODO:  istoriya odnogo goroda\n",
      "TODO:  foma-gordeev.gorod-zheltogo-djavola\n"
     ]
    }
   ],
   "source": [
    "dataFolderPath = '../data'\n",
    "booksFolderPath = dataFolderPath + '/books'\n",
    "booksJsonFilePath = booksFolderPath + '/books.json'\n",
    "booksJsonFile = open(booksJsonFilePath);\n",
    "booksJson = json.load(booksJsonFile);\n",
    "booksJsonFile.close();\n",
    "\n",
    "for bookJson in booksJson:\n",
    "    print(bookJson['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_direct(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            print('STRANGE TAG IN TAGGED!', file=sys.stderr)\n",
    "            continue\n",
    "        (word_id, word, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(word, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            print('NO LEMMA OR NO TOKEN!', file=sys.stderr)\n",
    "            continue\n",
    "        \n",
    "        if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "            lemma = num_replace(token)\n",
    "        spaces_after = ' '\n",
    "        if 'SpacesAfter=\\\\s\\\\s\\\\s' in misc:\n",
    "            spaces_after = '   '\n",
    "        elif 'SpacesAfter=\\\\s\\\\s' in misc:\n",
    "            spaces_after = '  '\n",
    "        elif 'SpacesAfter=\\\\s\\\\n' in misc:\n",
    "            spaces_after = '  \\n'\n",
    "        elif 'SpacesAfter=\\\\s' in misc:\n",
    "            spaces_after = ' '\n",
    "        elif 'SpacesAfter=\\\\n' in misc:\n",
    "            spaces_after = '\\n'        \n",
    "        elif 'SpaceAfter=No' in misc:\n",
    "            spaces_after = ''                    \n",
    "        else:\n",
    "            spaces_after = ' '\n",
    "            \n",
    "        chunk = word + spaces_after;\n",
    "        tagged_propn.append('%s_%s~%s|' % (lemma, pos, chunk))\n",
    "        \n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, modelfile=modelfile):    \n",
    "    print('Loading the model...')\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing text...')\n",
    "    output = process_direct(process_pipeline, text=text)\n",
    "    print('Text done')\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: TODO:  12-stuljev\n",
      "Opening original text from file... ./txt/11. 12-stuljev.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/11. 12-stuljev.lemmatized.txt\n",
      "Book done\n",
      "Name: TODO:  pohozhdeniya-bravogo-soldata-shveyka\n",
      "Opening original text from file... ./txt/12. pohozhdeniya-bravogo-soldata-shveyka.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/12. pohozhdeniya-bravogo-soldata-shveyka.lemmatized.txt\n",
      "Book done\n",
      "Name: TODO:  zapiski sumasshedshego\n",
      "Opening original text from file... ./txt/13. zapiski sumasshedshego.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/13. zapiski sumasshedshego.lemmatized.txt\n",
      "Book done\n",
      "Name: TODO:  istoriya odnogo goroda\n",
      "Opening original text from file... ./txt/14. istoriya odnogo goroda.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/14. istoriya odnogo goroda.lemmatized.txt\n",
      "Book done\n",
      "Name: TODO:  foma-gordeev.gorod-zheltogo-djavola\n",
      "Opening original text from file... ./txt/5. foma-gordeev.gorod-zheltogo-djavola.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/5. foma-gordeev.gorod-zheltogo-djavola.lemmatized.txt\n",
      "Book done\n"
     ]
    }
   ],
   "source": [
    "for bookJson in booksJson:\n",
    "    name = bookJson['name']\n",
    "    print('Name: ' + name)\n",
    "    original_text_path = bookJson['originalTextPath']\n",
    "    lemmatized_text_path = bookJson['lemmatizedTextPath']\n",
    "    print('Opening original text from file... ' + original_text_path)\n",
    "    original_text_file_handle = open(booksFolderPath + original_text_path, 'r', encoding='utf-8')\n",
    "    original_text = original_text_file_handle.read()\n",
    "    original_text_file_handle.close()\n",
    "    print('Lemmatizing text...')\n",
    "    lemmatized_text = lemmatize_text(original_text, modelfile)\n",
    "    \n",
    "    lemmatized_text_file_handle = open(booksFolderPath + lemmatized_text_path, 'w') \n",
    "    print('Writing lemmatized text to file... ' + lemmatized_text_path)\n",
    "    lemmatized_text_file_handle.write(lemmatized_text)\n",
    "    lemmatized_text_file_handle.close()\n",
    "    print('Book done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
