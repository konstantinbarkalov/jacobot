{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "#import os\n",
    "#import re\n",
    "import sys\n",
    "import json\n",
    "#import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "#modelfile = wget.download(udpipe_url)\n",
    "modelfile = 'udpipe_syntagrus.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Война и мир. Том I\n",
      "Идиот\n",
      "Мастер и Маргарита\n",
      "Палата № 6\n",
      "Повесть о разуме\n",
      "Что делать?\n",
      "Отцы и дети\n",
      "Обломов\n",
      "Старууха Изергиль. Карамола.\n",
      "Аэлита\n"
     ]
    }
   ],
   "source": [
    "dataFolderPath = '../data'\n",
    "booksFolderPath = dataFolderPath + '/books'\n",
    "booksJsonFilePath = booksFolderPath + '/books.json'\n",
    "booksJsonFile = open(booksJsonFilePath);\n",
    "booksJson = json.load(booksJsonFile);\n",
    "booksJsonFile.close();\n",
    "\n",
    "for bookJson in booksJson:\n",
    "    print(bookJson['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_direct(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            print('STRANGE TAG IN TAGGED!', file=sys.stderr)\n",
    "            continue\n",
    "        (word_id, word, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(word, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            print('NO LEMMA OR NO TOKEN!', file=sys.stderr)\n",
    "            continue\n",
    "        \n",
    "        if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "            lemma = num_replace(token)\n",
    "        spaces_after = ' '\n",
    "        if 'SpacesAfter=\\\\s\\\\s\\\\s' in misc:\n",
    "            spaces_after = '   '\n",
    "        elif 'SpacesAfter=\\\\s\\\\s' in misc:\n",
    "            spaces_after = '  '\n",
    "        elif 'SpacesAfter=\\\\s\\\\n' in misc:\n",
    "            spaces_after = '  \\n'\n",
    "        elif 'SpacesAfter=\\\\s' in misc:\n",
    "            spaces_after = ' '\n",
    "        elif 'SpacesAfter=\\\\n' in misc:\n",
    "            spaces_after = '\\n'        \n",
    "        elif 'SpaceAfter=No' in misc:\n",
    "            spaces_after = ''                    \n",
    "        else:\n",
    "            spaces_after = ' '\n",
    "            \n",
    "        chunk = word + spaces_after;\n",
    "        tagged_propn.append('%s_%s~%s|' % (lemma, pos, chunk))\n",
    "        \n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, modelfile=modelfile):    \n",
    "    print('Loading the model...')\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing text...')\n",
    "    output = process_direct(process_pipeline, text=text)\n",
    "    print('Text done')\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Война и мир. Том I\n",
      "Opening original text from file... ./txt/1. voyna-i-mir-tom-1.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/1. voyna-i-mir-tom-1.lemmatized.txt\n",
      "Book done\n",
      "Name: Идиот\n",
      "Opening original text from file... ./txt/2. idiot.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/2. idiot.lemmatized.txt\n",
      "Book done\n",
      "Name: Мастер и Маргарита\n",
      "Opening original text from file... ./txt/3. master-i-margarita.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/3. master-i-margarita.lemmatized.txt\n",
      "Book done\n",
      "Name: Палата № 6\n",
      "Opening original text from file... ./txt/4. palata-6-sbornik.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/4. palata-6-sbornik.lemmatized.txt\n",
      "Book done\n",
      "Name: Повесть о разуме\n",
      "Opening original text from file... ./txt/5. povest-o-razume.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lemmatized text to file... ./txt/5. povest-o-razume.lemmatized.txt\n",
      "Book done\n",
      "Name: Что делать?\n",
      "Opening original text from file... ./txt/6. сhto-delat.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/6. сhto-delat.lemmatized.txt\n",
      "Book done\n",
      "Name: Отцы и дети\n",
      "Opening original text from file... ./txt/7. otcy-i-dety.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/7. otcy-i-dety.lemmatized.txt\n",
      "Book done\n",
      "Name: Обломов\n",
      "Opening original text from file... ./txt/8. oblomov.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/8. oblomov.lemmatized.txt\n",
      "Book done\n",
      "Name: Старууха Изергиль. Карамола.\n",
      "Opening original text from file... ./txt/9. staruha-isergil.karamola.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/9. staruha-isergil.karamola.lemmatized.txt\n",
      "Book done\n",
      "Name: Аэлита\n",
      "Opening original text from file... ./txt/10. aelita.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/10. aelita.lemmatized.txt\n",
      "Book done\n"
     ]
    }
   ],
   "source": [
    "for bookJson in booksJson:\n",
    "    name = bookJson['name']\n",
    "    print('Name: ' + name)\n",
    "    original_text_path = bookJson['originalTextPath']\n",
    "    lemmatized_text_path = bookJson['lemmatizedTextPath']\n",
    "    print('Opening original text from file... ' + original_text_path)\n",
    "    original_text_file_handle = open(booksFolderPath + original_text_path, 'r', encoding='utf-8')\n",
    "    original_text = original_text_file_handle.read()\n",
    "    original_text_file_handle.close()\n",
    "    print('Lemmatizing text...')\n",
    "    lemmatized_text = lemmatize_text(original_text, modelfile)\n",
    "    \n",
    "    lemmatized_text_file_handle = open(booksFolderPath + lemmatized_text_path, 'w') \n",
    "    print('Writing lemmatized text to file... ' + lemmatized_text_path)\n",
    "    lemmatized_text_file_handle.write(lemmatized_text)\n",
    "    lemmatized_text_file_handle.close()\n",
    "    print('Book done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
