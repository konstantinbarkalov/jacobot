{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "#import os\n",
    "#import re\n",
    "import sys\n",
    "import json\n",
    "#import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "#modelfile = wget.download(udpipe_url)\n",
    "modelfile = 'udpipe_syntagrus.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Война и мир. Том 1\n",
      "Идиот\n",
      "Мастер и Маргарита\n",
      "Палата № 6\n",
      "Фома Гордеев\n",
      "Что делать?\n",
      "Отцы и дети\n",
      "Обломов\n",
      "Старуха Изергиль\n",
      "Аэлита\n",
      "12 стульев\n",
      "Похождения бравого солдата Швейка\n",
      "Записки сумасшедшего\n",
      "История одного города\n",
      "Большие Безобразия Маленького Папы\n",
      "Белые ночи\n",
      "Чушь собачья\n",
      "Дар Шаванахолы\n",
      "Игра в бисер\n",
      "Король, дама, валет\n",
      "Лолита\n",
      "О дивный новый мир\n",
      "Смерть африканского охотника\n",
      "Записки Хендрика Груна из амстердамской богадельни\n",
      "Деловые люди\n",
      "Красота\n",
      "Книга песка\n",
      "Москва — Петушки\n",
      "Непобедимое Солнце\n",
      "Не позвать ли нам Дживса?\n",
      "Пейзаж, нарисованный чаем\n",
      "Понедельник начинается в субботу\n",
      "Сказка о тройке\n",
      "Последнее путешествие Ийона Тихого\n",
      "С нами бот\n",
      "Юмористические рассказы\n",
      "Заповедник\n",
      "Америка\n",
      "Замок\n",
      "Трое в лодке, не считая собаки\n",
      "Золотой теленок\n",
      "Сукины дети\n",
      "Карамора\n",
      "Приключения барона Мюнхаузена\n",
      "Июнь\n",
      "Собачье сердце\n",
      "Вавилонские хроники / Обретение Энкиду\n"
     ]
    }
   ],
   "source": [
    "dataFolderPath = '../data'\n",
    "booksFolderPath = dataFolderPath + '/books'\n",
    "booksJsonFilePath = booksFolderPath + '/books.json'\n",
    "booksJsonFile = open(booksJsonFilePath);\n",
    "booksJson = json.load(booksJsonFile);\n",
    "booksJsonFile.close();\n",
    "\n",
    "for bookJson in booksJson:\n",
    "    print(bookJson['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    return newtoken\n",
    "\n",
    "def clean_token(token, misc):\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token\n",
    "\n",
    "\n",
    "def clean_lemma(lemma, pos):\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_direct(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10:\n",
    "            print('STRANGE TAG IN TAGGED!', file=sys.stderr)\n",
    "            continue\n",
    "        (word_id, word, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "        token = clean_token(word, misc)\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "        if not lemma or not token:\n",
    "            print('NO LEMMA OR NO TOKEN!', file=sys.stderr)\n",
    "            continue\n",
    "        \n",
    "        if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "            lemma = num_replace(token)\n",
    "        spaces_after = ' '\n",
    "        if 'SpacesAfter=\\\\s\\\\s\\\\s' in misc:\n",
    "            spaces_after = '   '\n",
    "        elif 'SpacesAfter=\\\\s\\\\s' in misc:\n",
    "            spaces_after = '  '\n",
    "        elif 'SpacesAfter=\\\\s\\\\n' in misc:\n",
    "            spaces_after = '  \\n'\n",
    "        elif 'SpacesAfter=\\\\s' in misc:\n",
    "            spaces_after = ' '\n",
    "        elif 'SpacesAfter=\\\\n' in misc:\n",
    "            spaces_after = '\\n'        \n",
    "        elif 'SpaceAfter=No' in misc:\n",
    "            spaces_after = ''                    \n",
    "        else:\n",
    "            spaces_after = ' '\n",
    "            \n",
    "        chunk = word + spaces_after;\n",
    "        tagged_propn.append('%s_%s~%s|' % (lemma, pos, chunk))\n",
    "        \n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text, modelfile=modelfile):    \n",
    "    print('Loading the model...')\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    print('Processing text...')\n",
    "    output = process_direct(process_pipeline, text=text)\n",
    "    print('Text done')\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Война и мир. Том 1\n",
      "Opening original text from file... ./txt/voyna-i-mir-tom-1.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/voyna-i-mir-tom-1.lemmatized.txt\n",
      "Book done\n",
      "Name: Идиот\n",
      "Opening original text from file... ./txt/idiot.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/idiot.lemmatized.txt\n",
      "Book done\n",
      "Name: Мастер и Маргарита\n",
      "Opening original text from file... ./txt/master-i-margarita.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/master-i-margarita.lemmatized.txt\n",
      "Book done\n",
      "Name: Палата № 6\n",
      "Opening original text from file... ./txt/palata-6-sbornik.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/palata-6-sbornik.lemmatized.txt\n",
      "Book done\n",
      "Name: Фома Гордеев\n",
      "Opening original text from file... ./txt/foma-gordeev.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/foma-gordeev.lemmatized.txt\n",
      "Book done\n",
      "Name: Что делать?\n",
      "Opening original text from file... ./txt/сhto-delat.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/сhto-delat.lemmatized.txt\n",
      "Book done\n",
      "Name: Отцы и дети\n",
      "Opening original text from file... ./txt/otcy-i-dety.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/otcy-i-dety.lemmatized.txt\n",
      "Book done\n",
      "Name: Обломов\n",
      "Opening original text from file... ./txt/oblomov.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/oblomov.lemmatized.txt\n",
      "Book done\n",
      "Name: Старуха Изергиль\n",
      "Opening original text from file... ./txt/staruha-isergil.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/staruha-isergil.lemmatized.txt\n",
      "Book done\n",
      "Name: Аэлита\n",
      "Opening original text from file... ./txt/aelita.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/aelita.lemmatized.txt\n",
      "Book done\n",
      "Name: 12 стульев\n",
      "Opening original text from file... ./txt/12-stuljev.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/12-stuljev.lemmatized.txt\n",
      "Book done\n",
      "Name: Похождения бравого солдата Швейка\n",
      "Opening original text from file... ./txt/pohozhdeniya-bravogo-soldata-shveyka.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/pohozhdeniya-bravogo-soldata-shveyka.lemmatized.txt\n",
      "Book done\n",
      "Name: Записки сумасшедшего\n",
      "Opening original text from file... ./txt/zapiski-sumasshedshego.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/zapiski-sumasshedshego.lemmatized.txt\n",
      "Book done\n",
      "Name: История одного города\n",
      "Opening original text from file... ./txt/istoriya-odnogo-goroda.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/istoriya-odnogo-goroda.lemmatized.txt\n",
      "Book done\n",
      "Name: Большие Безобразия Маленького Папы\n",
      "Opening original text from file... ./txt/bolshie-bezobraziya-malenkogo-papy.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/bolshie-bezobraziya-malenkogo-papy.lemmatized.txt\n",
      "Book done\n",
      "Name: Белые ночи\n",
      "Opening original text from file... ./txt/belye-nochi.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/belye-nochi.lemmatized.txt\n",
      "Book done\n",
      "Name: Чушь собачья\n",
      "Opening original text from file... ./txt/chush-sobachya.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/chush-sobachya.lemmatized.txt\n",
      "Book done\n",
      "Name: Дар Шаванахолы\n",
      "Opening original text from file... ./txt/dar-Shavanaholy.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/dar-Shavanaholy.lemmatized.txt\n",
      "Book done\n",
      "Name: Игра в бисер\n",
      "Opening original text from file... ./txt/igra-v-biser.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/igra-v-biser.lemmatized.txt\n",
      "Book done\n",
      "Name: Король, дама, валет\n",
      "Opening original text from file... ./txt/korol-dama-valet.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/korol-dama-valet.lemmatized.txt\n",
      "Book done\n",
      "Name: Лолита\n",
      "Opening original text from file... ./txt/lolita.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/lolita.lemmatized.txt\n",
      "Book done\n",
      "Name: О дивный новый мир\n",
      "Opening original text from file... ./txt/o-divnyy-novyy-mir.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/o-divnyy-novyy-mir.lemmatized.txt\n",
      "Book done\n",
      "Name: Смерть африканского охотника\n",
      "Opening original text from file... ./txt/smert-afrikanskogo-ohotnika.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/smert-afrikanskogo-ohotnika.lemmatized.txt\n",
      "Book done\n",
      "Name: Записки Хендрика Груна из амстердамской богадельни\n",
      "Opening original text from file... ./txt/zapiski-Hendrika-Gruna-iz-amsterdamskoy-bogadelni.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/zapiski-Hendrika-Gruna-iz-amsterdamskoy-bogadelni.lemmatized.txt\n",
      "Book done\n",
      "Name: Деловые люди\n",
      "Opening original text from file... ./txt/delovye-lyudi.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/delovye-lyudi.lemmatized.txt\n",
      "Book done\n",
      "Name: Красота\n",
      "Opening original text from file... ./txt/krasota.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/krasota.lemmatized.txt\n",
      "Book done\n",
      "Name: Книга песка\n",
      "Opening original text from file... ./txt/kniga-peska.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/kniga-peska.lemmatized.txt\n",
      "Book done\n",
      "Name: Москва — Петушки\n",
      "Opening original text from file... ./txt/moskva-petushki.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/moskva-petushki.lemmatized.txt\n",
      "Book done\n",
      "Name: Непобедимое Солнце\n",
      "Opening original text from file... ./txt/nepobedimoe-solnce.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/nepobedimoe-solnce.lemmatized.txt\n",
      "Book done\n",
      "Name: Не позвать ли нам Дживса?\n",
      "Opening original text from file... ./txt/ne-pozvat-li-nam-Dzhivsa.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/ne-pozvat-li-nam-Dzhivsa.lemmatized.txt\n",
      "Book done\n",
      "Name: Пейзаж, нарисованный чаем\n",
      "Opening original text from file... ./txt/peyzazh-narisovannyy-chaem.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/peyzazh-narisovannyy-chaem.lemmatized.txt\n",
      "Book done\n",
      "Name: Понедельник начинается в субботу\n",
      "Opening original text from file... ./txt/ponedelnik-nachinaetsya-v-subbotu.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/ponedelnik-nachinaetsya-v-subbotu.lemmatized.txt\n",
      "Book done\n",
      "Name: Сказка о тройке\n",
      "Opening original text from file... ./txt/skazka-o-troyke.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/skazka-o-troyke.lemmatized.txt\n",
      "Book done\n",
      "Name: Последнее путешествие Ийона Тихого\n",
      "Opening original text from file... ./txt/poslednee-puteshestvie-Iyona-Tihogo.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/poslednee-puteshestvie-Iyona-Tihogo.lemmatized.txt\n",
      "Book done\n",
      "Name: С нами бот\n",
      "Opening original text from file... ./txt/s-nami-bot.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/s-nami-bot.lemmatized.txt\n",
      "Book done\n",
      "Name: Юмористические рассказы\n",
      "Opening original text from file... ./txt/yumoristicheskie-rasskazy(Teffi).src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/yumoristicheskie-rasskazy(Teffi).lemmatized.txt\n",
      "Book done\n",
      "Name: Заповедник\n",
      "Opening original text from file... ./txt/zapovednik.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/zapovednik.lemmatized.txt\n",
      "Book done\n",
      "Name: Америка\n",
      "Opening original text from file... ./txt/аmerika.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/аmerika.lemmatized.txt\n",
      "Book done\n",
      "Name: Замок\n",
      "Opening original text from file... ./txt/zamok.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/zamok.lemmatized.txt\n",
      "Book done\n",
      "Name: Трое в лодке, не считая собаки\n",
      "Opening original text from file... ./txt/troe-v-lodke-ne-schitaya-sobaki.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/troe-v-lodke-ne-schitaya-sobaki.lemmatized.txt\n",
      "Book done\n",
      "Name: Золотой теленок\n",
      "Opening original text from file... ./txt/zolotoy-telenok.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/zolotoy-telenok.lemmatized.txt\n",
      "Book done\n",
      "Name: Сукины дети\n",
      "Opening original text from file... ./txt/sukiny-deti.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/sukiny-deti.lemmatized.txt\n",
      "Book done\n",
      "Name: Карамора\n",
      "Opening original text from file... ./txt/karamora.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/karamora.lemmatized.txt\n",
      "Book done\n",
      "Name: Приключения барона Мюнхаузена\n",
      "Opening original text from file... ./txt/priklyucheniya-barona-Myunghauzena.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/priklyucheniya-barona-Myunghauzena.lemmatized.txt\n",
      "Book done\n",
      "Name: Июнь\n",
      "Opening original text from file... ./txt/iyun.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n",
      "NO LEMMA OR NO TOKEN!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text done\n",
      "Writing lemmatized text to file... ./txt/iyun.lemmatized.txt\n",
      "Book done\n",
      "Name: Собачье сердце\n",
      "Opening original text from file... ./txt/sobache-serdce.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/sobache-serdce.lemmatized.txt\n",
      "Book done\n",
      "Name: Вавилонские хроники / Обретение Энкиду\n",
      "Opening original text from file... ./txt/vavilonskie-hroniki-obretenie-enkidu.src.txt\n",
      "Lemmatizing text...\n",
      "Loading the model...\n",
      "Processing text...\n",
      "Text done\n",
      "Writing lemmatized text to file... ./txt/vavilonskie-hroniki-obretenie-enkidu.lemmatized.txt\n",
      "Book done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NO LEMMA OR NO TOKEN!\n"
     ]
    }
   ],
   "source": [
    "for bookJson in booksJson:\n",
    "    name = bookJson['name']\n",
    "    print('Name: ' + name)\n",
    "    original_text_path = bookJson['originalTextPath']\n",
    "    lemmatized_text_path = bookJson['lemmatizedTextPath']\n",
    "    print('Opening original text from file... ' + original_text_path)\n",
    "    original_text_file_handle = open(booksFolderPath + original_text_path, 'r', encoding='utf-8')\n",
    "    original_text = original_text_file_handle.read()\n",
    "    original_text_file_handle.close()\n",
    "    print('Lemmatizing text...')\n",
    "    lemmatized_text = lemmatize_text(original_text, modelfile)\n",
    "    \n",
    "    lemmatized_text_file_handle = open(booksFolderPath + lemmatized_text_path, 'w') \n",
    "    print('Writing lemmatized text to file... ' + lemmatized_text_path)\n",
    "    lemmatized_text_file_handle.write(lemmatized_text)\n",
    "    lemmatized_text_file_handle.close()\n",
    "    print('Book done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
